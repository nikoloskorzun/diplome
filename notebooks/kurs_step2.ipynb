{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5cda2cc-2500-49e2-9183-dc99afe149c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "libs_path = os.path.abspath(os.path.join(current_dir, \"..\", \"libs\"))\n",
    "if libs_path not in sys.path:\n",
    "    sys.path.append(libs_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 3)\n",
    "import data_processing as dp\n",
    "import models as myML\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9afc96-e388-4566-9f0c-59334d7db28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Flatten, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Установка seed для воспроизводимости\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = pd.read_excel(\"../datasets_ref/source.xls\")\n",
    "\n",
    "dataset[\"temp_Class\"] = np.uint(np.bool_(dataset[\"Class\"]))\n",
    "dataset[\"Class\"] = dataset[\"temp_Class\"]\n",
    "dataset = dataset.drop(columns=[\"temp_Class\"])\n",
    "\n",
    "dataset[\"power, W\"] = dataset[\"Ubs,V\"] * dataset[\"Ibs,A\"]\n",
    "dataset[\"D+\"] = np.sqrt(np.power(dataset[\"TR11,C\"],2) + np.power(dataset[\"TR13,C\"], 2) + np.power(dataset[\"TR15,C\"], 2))\n",
    "dataset[\"D-\"] = np.sqrt(np.power(dataset[\"TR12,C\"],2) + np.power(dataset[\"TR14,C\"], 2) + np.power(dataset[\"TR16,C\"], 2))\n",
    "dataset[\"D\"] = np.sqrt(np.power(dataset[\"D+\"],2) + np.power(dataset[\"D-\"], 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5174bdb7-7c5a-42f7-8eff-4d76d1a6185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ГЕНЕТИЧЕСКИЙ АЛГОРИТМ (DEAP) ===\n",
      "gen\tnevals\tmin     \n",
      "0  \t10    \t0.254407\n",
      "1  \t9     \t0.254407\n",
      "2  \t6     \t0.254407\n",
      "3  \t7     \t0.254407\n",
      "4  \t6     \t0.254407\n",
      "5  \t5     \t0.253288\n",
      "\n",
      "Лучшие гиперпараметры: conv_filters=256, kernel_size=7, lstm_units=128, dense_neurons=64, optimizer=0\n",
      "\n",
      "=== ФИНАЛЬНОЕ ОБУЧЕНИЕ АВТОКОДИРОВЩИКА ===\n",
      "Epoch 1/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.7869\n",
      "Epoch 2/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3215 \n",
      "Epoch 3/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3163\n",
      "Epoch 4/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3127 \n",
      "Epoch 5/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2797\n",
      "Epoch 6/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2661 \n",
      "Epoch 7/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2587\n",
      "Epoch 8/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2527\n",
      "Epoch 9/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2458 \n",
      "Epoch 10/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2384 \n",
      "Epoch 11/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2333 \n",
      "Epoch 12/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2316 \n",
      "Epoch 13/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2323 \n",
      "Epoch 14/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2255 \n",
      "Epoch 15/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2238 \n",
      "Epoch 16/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2184\n",
      "Epoch 17/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2159 \n",
      "Epoch 18/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2142 \n",
      "Epoch 19/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2149 \n",
      "Epoch 20/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2099 \n",
      "Epoch 21/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2092 \n",
      "Epoch 22/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2064 \n",
      "Epoch 23/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2085 \n",
      "Epoch 24/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2085 \n",
      "Epoch 25/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2068 \n",
      "Epoch 26/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2017 \n",
      "Epoch 27/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2006 \n",
      "Epoch 28/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1971\n",
      "Epoch 29/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1955\n",
      "Epoch 30/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1938 \n",
      "Epoch 31/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1939\n",
      "Epoch 32/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1929 \n",
      "Epoch 33/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1923 \n",
      "Epoch 34/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1923 \n",
      "Epoch 35/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1912\n",
      "Epoch 36/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1903 \n",
      "Epoch 37/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1906 \n",
      "Epoch 38/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1887 \n",
      "Epoch 39/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1878 \n",
      "Epoch 40/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1867 \n",
      "Epoch 41/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1860 \n",
      "Epoch 42/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1856 \n",
      "Epoch 43/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1855 \n",
      "Epoch 44/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1851 \n",
      "Epoch 45/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1839 \n",
      "Epoch 46/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1832 \n",
      "Epoch 47/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1833 \n",
      "Epoch 48/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1822\n",
      "Epoch 49/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1840 \n",
      "Epoch 50/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1814\n",
      "\n",
      "=== ЗАГРУЗКА РАЗМЕЧЕННЫХ ДАННЫХ ===\n",
      "\n",
      "=== ОБУЧЕНИЕ БАЗОВОГО КЛАССИФИКАТОРА (БЭГГИНГ BiLSTM) ===\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step  \n",
      "\n",
      "=== ОПРЕДЕЛЕНИЕ АНОМАЛИЙ АВТОКОДИРОВЩИКОМ ===\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step   \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Порог ошибки (95-й перцентиль на обучении): 1.0969\n",
      "\n",
      "=== СРАВНЕНИЕ МЕТОК: КЛАССИФИКАТОР vs АВТОКОДИРОВЩИК ===\n",
      "\n",
      "--- Метки классификатора (истинные) ---\n",
      "[[700   4]\n",
      " [  9  88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       704\n",
      "           1       0.96      0.91      0.93        97\n",
      "\n",
      "    accuracy                           0.98       801\n",
      "   macro avg       0.97      0.95      0.96       801\n",
      "weighted avg       0.98      0.98      0.98       801\n",
      "\n",
      "\n",
      "--- Метки автокодировщика (аномалии) ---\n",
      "[[690  14]\n",
      " [ 66  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95       704\n",
      "           1       0.69      0.32      0.44        97\n",
      "\n",
      "    accuracy                           0.90       801\n",
      "   macro avg       0.80      0.65      0.69       801\n",
      "weighted avg       0.89      0.90      0.88       801\n",
      "\n",
      "\n",
      "Процент совпадений меток между классификатором и автокодировщиком: 90.39%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== ФИКСАЦИЯ СЛУЧАЙНЫХ ЧИСЕЛ ====================\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ==================== ПАРАМЕТРЫ (ЭМПИРИЧЕСКИ ФИКСИРУЕМ) ====================\n",
    "WINDOW_SIZE = 10          # размер окна временного ряда\n",
    "CONV_LAYERS = 2           # число свёрточных слоёв (фиксировано)\n",
    "RECURRENT_LAYERS = 1      # число рекуррентных слоёв (будет один Bidirectional LSTM)\n",
    "DENSE_LAYERS = 2          # число полносвязных слоёв в энкодере/декодере\n",
    "LATENT_DIM = 32           # размерность бутылочного горлышка\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS_AUTO = 20         # эпох при поиске гиперпараметров\n",
    "EPOCHS_FINAL = 50        # эпох при финальном обучении\n",
    "N_ESTIMATORS = 5         # число моделей в бэггинге\n",
    "\n",
    "# ==================== ЗАГРУЗКА ДАННЫХ И ФОРМИРОВАНИЕ ОКОН ====================\n",
    "def load_data(file_path, window_size, has_labels=True, label_col='Class'):\n",
    "    \"\"\"\n",
    "    Загружает CSV, нормализует признаки, создаёт скользящие окна.\n",
    "    Если есть столбец label_col, возвращает также метки (для классификатора).\n",
    "    \"\"\"\n",
    "    if has_labels:\n",
    "        df = dataset\n",
    "    else:\n",
    "        df = dataset.drop(columns=[\"Class\"])\n",
    "\n",
    "    \n",
    "    if has_labels:\n",
    "        y = df[label_col].values\n",
    "        X = df.drop(columns=[label_col]).values\n",
    "    else:\n",
    "        X = df.values\n",
    "        y = None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Формирование окон\n",
    "    X_windows, y_windows = [], []\n",
    "    for i in range(len(X_scaled) - window_size + 1):\n",
    "        X_windows.append(X_scaled[i:i+window_size])\n",
    "        if y is not None:\n",
    "            # метка для окна – берём метку последнего момента (или большинство)\n",
    "            y_windows.append(y[i+window_size-1])\n",
    "    X_windows = np.array(X_windows)\n",
    "    y_windows = np.array(y_windows) if y_windows else None\n",
    "    return X_windows, y_windows, scaler\n",
    "\n",
    "# ==================== ПОСТРОЕНИЕ АВТОКОДИРОВЩИКА ====================\n",
    "def create_autoencoder(input_shape, params):\n",
    "    \"\"\"\n",
    "    params: (conv_filters, kernel_size, lstm_units, dense_neurons, optimizer_idx)\n",
    "    Возвращает модель автокодировщика.\n",
    "    Архитектура фиксирована: Conv1D x2 -> BiLSTM -> Dense x2 -> Bottleneck -> Dense x2 -> Reshape\n",
    "    \"\"\"\n",
    "    conv_filters, kernel_size, lstm_units, dense_neurons, opt_idx = params\n",
    "    optimizer_list = ['adam', 'sgd', 'rmsprop', 'adagrad']\n",
    "    optimizer = optimizer_list[opt_idx]\n",
    "\n",
    "    # Энкодер\n",
    "    encoder_input = layers.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    for _ in range(CONV_LAYERS):\n",
    "        x = layers.Conv1D(filters=conv_filters, kernel_size=kernel_size,\n",
    "                          padding='same', activation='relu')(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False))(x)\n",
    "    for _ in range(DENSE_LAYERS):\n",
    "        x = layers.Dense(dense_neurons, activation='relu')(x)\n",
    "    encoder_output = layers.Dense(LATENT_DIM, activation='relu')(x)\n",
    "\n",
    "    # Декодер (полносвязный, симметричный)\n",
    "    d = encoder_output\n",
    "    for _ in range(DENSE_LAYERS):\n",
    "        d = layers.Dense(dense_neurons, activation='relu')(d)\n",
    "    # Восстанавливаем исходную размерность (все временные шаги × признаки)\n",
    "    decoded = layers.Dense(input_shape[0] * input_shape[1], activation='linear')(d)\n",
    "    decoder_output = layers.Reshape(input_shape)(decoded)\n",
    "\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "# ==================== ФУНКЦИЯ ПРИСПОСОБЛЕННОСТИ (ДЛЯ DEAP) ====================\n",
    "def evaluate_autoencoder(individual, X_train, X_val):\n",
    "    \"\"\"Обучает автокодировщик с параметрами individual и возвращает val_loss.\"\"\"\n",
    "    params = tuple(individual)\n",
    "    try:\n",
    "        input_shape = X_train.shape[1:]  # (window, n_features)\n",
    "        model = create_autoencoder(input_shape, params)\n",
    "        history = model.fit(\n",
    "            X_train, X_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS_AUTO,\n",
    "            validation_data=(X_val, X_val),\n",
    "            verbose=0,\n",
    "            callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "        )\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        return val_loss,\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обучении с params {params}: {e}\")\n",
    "        return 1000.0,   # штраф\n",
    "\n",
    "# ==================== ГЕНЕТИЧЕСКИЙ АЛГОРИТМ (DEAP) ====================\n",
    "def setup_deap(X_train, X_val):\n",
    "    \"\"\"Настраивает toolbox DEAP для задачи минимизации val_loss.\"\"\"\n",
    "    # Пространство поиска\n",
    "    conv_filters_choices = [32, 64, 128, 256]\n",
    "    kernel_choices = [3, 5, 7]\n",
    "    lstm_units_choices = [32, 64, 128, 256]\n",
    "    dense_neurons_choices = [32, 64, 128, 256]\n",
    "    optimizer_choices = [0, 1, 2, 3]  # adam, sgd, rmsprop, adagrad\n",
    "\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_conv_filters\", random.choice, conv_filters_choices)\n",
    "    toolbox.register(\"attr_kernel\", random.choice, kernel_choices)\n",
    "    toolbox.register(\"attr_lstm_units\", random.choice, lstm_units_choices)\n",
    "    toolbox.register(\"attr_dense_neurons\", random.choice, dense_neurons_choices)\n",
    "    toolbox.register(\"attr_optimizer\", random.choice, optimizer_choices)\n",
    "\n",
    "    toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                     (toolbox.attr_conv_filters, toolbox.attr_kernel,\n",
    "                      toolbox.attr_lstm_units, toolbox.attr_dense_neurons,\n",
    "                      toolbox.attr_optimizer), n=1)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutUniformInt,\n",
    "                     low=[32, 3, 32, 32, 0],\n",
    "                     up=[256, 7, 256, 256, 3],\n",
    "                     indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    toolbox.register(\"evaluate\", evaluate_autoencoder, X_train=X_train, X_val=X_val)\n",
    "\n",
    "    return toolbox\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Одиночный классификатор BiLSTM + Dense для бэггинга (работает с 3D).\"\"\"\n",
    "    def __init__(self, lstm_units=64, dense_neurons=32, epochs=30, batch_size=64):\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dense_neurons = dense_neurons\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.classes_ = np.array([0, 1])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Разрешаем многомерные входные данные (allow_nd=True)\n",
    "        X, y = check_X_y(X, y, allow_nd=True, multi_output=False, dtype=np.float32)\n",
    "        # Если вдруг пришло 2D – превращаем в 3D (один временной шаг)\n",
    "        if X.ndim == 2:\n",
    "            X = X.reshape(X.shape[0], 1, -1)\n",
    "        input_shape = X.shape[1:]\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(self.lstm_units, return_sequences=False),\n",
    "                input_shape=input_shape\n",
    "            ),\n",
    "            tf.keras.layers.Dense(self.dense_neurons, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'model')\n",
    "        X = check_array(X, allow_nd=True, dtype=np.float32)\n",
    "        if X.ndim == 2:\n",
    "            X = X.reshape(X.shape[0], 1, -1)\n",
    "        proba = self.model.predict(X)\n",
    "        return (proba > 0.5).astype(int).flatten()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, 'model')\n",
    "        X = check_array(X, allow_nd=True, dtype=np.float32)\n",
    "        if X.ndim == 2:\n",
    "            X = X.reshape(X.shape[0], 1, -1)\n",
    "        proba = self.model.predict(X)\n",
    "        return np.hstack([1 - proba, proba])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'lstm_units': self.lstm_units,\n",
    "            'dense_neurons': self.dense_neurons,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class BaggingBiLSTMEnsemble:\n",
    "    \"\"\"\n",
    "    Ансамбль из N моделей BiLSTMClassifier, обучаемых на bootstrap-подвыборках.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=5, lstm_units=64, dense_neurons=32,\n",
    "                 epochs=30, batch_size=64, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dense_neurons = dense_neurons\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.classes_ = np.array([0, 1])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучаем n_estimators моделей на bootstrap-копиях данных.\"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        self.models = []\n",
    "        for i in range(self.n_estimators):\n",
    "            # Генерируем bootstrap-индексы (с повторениями)\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_boot = X[indices]\n",
    "            y_boot = y[indices]\n",
    "\n",
    "            model = BiLSTMClassifier(\n",
    "                lstm_units=self.lstm_units,\n",
    "                dense_neurons=self.dense_neurons,\n",
    "                epochs=self.epochs,\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "            model.fit(X_boot, y_boot)\n",
    "            self.models.append(model)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Усреднённые вероятности по всем моделям ансамбля.\"\"\"\n",
    "        probas = np.array([model.predict_proba(X) for model in self.models])\n",
    "        return np.mean(probas, axis=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Метка на основе усреднённой вероятности > 0.5.\"\"\"\n",
    "        proba = self.predict_proba(X)[:, 1]  # вероятность класса 1\n",
    "        return (proba > 0.5).astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "def create_bagging_ensemble(X_train, y_train, n_estimators=N_ESTIMATORS):\n",
    "    \"\"\"Создаёт и обучает ансамбль BiLSTMClassifier с бэггингом.\"\"\"\n",
    "    ensemble = BaggingBiLSTMEnsemble(\n",
    "        n_estimators=n_estimators,\n",
    "        lstm_units=64,          \n",
    "        dense_neurons=32,\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    return ensemble\n",
    "\n",
    "# ==================== АНОМАЛИИ ПО АВТОКОДИРОВЩИКУ ====================\n",
    "def get_anomaly_scores(autoencoder, X, threshold_percentile=95):\n",
    "    \"\"\"Вычисляет ошибку реконструкции MSE для каждого окна.\"\"\"\n",
    "    reconstructions = autoencoder.predict(X)\n",
    "    mse = np.mean(np.square(X - reconstructions), axis=(1,2)) #//слабая точка, вместо этой хрени нужно на основе важности + \n",
    "    return mse\n",
    "\n",
    "def detect_anomalies(autoencoder, X_train, X_test, threshold_percentile=95):\n",
    "    \"\"\"Определяет порог на обучающей выборке и выдаёт бинарные метки для теста.\"\"\"\n",
    "    train_errors = get_anomaly_scores(autoencoder, X_train)\n",
    "    threshold = np.percentile(train_errors, threshold_percentile)\n",
    "    test_errors = get_anomaly_scores(autoencoder, X_test)\n",
    "    preds = (test_errors > threshold).astype(int)\n",
    "    return preds, threshold\n",
    "\n",
    "# ==================== ОСНОВНОЙ ПРОЦЕСС ====================\n",
    "if True:\n",
    "    data_path_unlabeled = 'unlabeled'\n",
    "    data_path_labeled = 'labeled'\n",
    "\n",
    "    \"\"\"\n",
    "    data_path_unlabeled: путь к неразмеченному файлу (для обучения автокодировщика)\n",
    "    data_path_labeled:   путь к размеченному файлу (для обучения и тестирования классификатора)\n",
    "    \"\"\"\n",
    "    # 1. Загружаем неразмеченные данные (только признаки, без 'class')\n",
    "    X_unlabeled, _, _ = load_data(data_path_unlabeled, WINDOW_SIZE, has_labels=False)\n",
    "\n",
    "    # Разбиваем на train/val для GA (на неразмеченных данных)\n",
    "    X_train_auto, X_val_auto = train_test_split(X_unlabeled, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    # 2. ГЕНЕТИЧЕСКИЙ ПОИСК ГИПЕРПАРАМЕТРОВ АВТОКОДИРОВЩИКА\n",
    "    print(\"=== ГЕНЕТИЧЕСКИЙ АЛГОРИТМ (DEAP) ===\")\n",
    "    toolbox = setup_deap(X_train_auto, X_val_auto)\n",
    "    pop = toolbox.population(n=10)   # размер популяции\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", np.min)\n",
    "\n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n",
    "                                   ngen=5, stats=stats, halloffame=hof,\n",
    "                                   verbose=True)\n",
    "\n",
    "    best_params = tuple(hof[0])\n",
    "    print(f\"\\nЛучшие гиперпараметры: conv_filters={best_params[0]}, kernel_size={best_params[1]}, \"\n",
    "          f\"lstm_units={best_params[2]}, dense_neurons={best_params[3]}, optimizer={best_params[4]}\")\n",
    "    \n",
    "    # 3. ФИНАЛЬНОЕ ОБУЧЕНИЕ АВТОКОДИРОВЩИКА НА ВСЕХ НЕРАЗМЕЧЕННЫХ ДАННЫХ\n",
    "    print(\"\\n=== ФИНАЛЬНОЕ ОБУЧЕНИЕ АВТОКОДИРОВЩИКА ===\")\n",
    "    input_shape = X_unlabeled.shape[1:]\n",
    "    final_autoencoder = create_autoencoder(input_shape, best_params)\n",
    "    final_autoencoder.fit(\n",
    "        X_unlabeled, X_unlabeled,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_FINAL,\n",
    "        verbose=1,\n",
    "        callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "    # 4. ЗАГРУЗКА РАЗМЕЧЕННЫХ ДАННЫХ (ДЛЯ КЛАССИФИКАТОРА И ТЕСТИРОВАНИЯ)\n",
    "    print(\"\\n=== ЗАГРУЗКА РАЗМЕЧЕННЫХ ДАННЫХ ===\")\n",
    "    X_labeled, y_labeled, _ = load_data(data_path_labeled, WINDOW_SIZE, has_labels=True)\n",
    "    X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "        X_labeled, y_labeled, test_size=0.3, random_state=SEED, stratify=y_labeled)\n",
    "\n",
    "    # 5. ОБУЧЕНИЕ БАЗОВОГО КЛАССИФИКАТОРА (БЭГГИНГ BiLSTM)\n",
    "    print(\"\\n=== ОБУЧЕНИЕ БАЗОВОГО КЛАССИФИКАТОРА (БЭГГИНГ BiLSTM) ===\")\n",
    "    clf = create_bagging_ensemble(X_train_class, y_train_class, n_estimators=N_ESTIMATORS)\n",
    "    y_pred_class = clf.predict(X_test_class)\n",
    "    # Предсказания классификатора\n",
    "\n",
    "    # 6. АНОМАЛИИ ПО АВТОКОДИРОВЩИКУ НА ТЕСТЕ\n",
    "    print(\"\\n=== ОПРЕДЕЛЕНИЕ АНОМАЛИЙ АВТОКОДИРОВЩИКОМ ===\")\n",
    "    # Используем обучающую выборку классификатора для вычисления порога (она размечена, но метки не используем)\n",
    "    # Это допустимо – автокодировщик видит только X_train_class (без y)\n",
    "    y_pred_auto, threshold = detect_anomalies(final_autoencoder, X_train_class, X_test_class, threshold_percentile=95)\n",
    "    print(f\"Порог ошибки (95-й перцентиль на обучении): {threshold:.4f}\")\n",
    "\n",
    "    # 7. СРАВНЕНИЕ РЕЗУЛЬТАТОВ\n",
    "    print(\"\\n=== СРАВНЕНИЕ МЕТОК: КЛАССИФИКАТОР vs АВТОКОДИРОВЩИК ===\")\n",
    "    print(\"\\n--- Метки классификатора (истинные) ---\")\n",
    "    print(confusion_matrix(y_test_class, y_pred_class))\n",
    "    print(classification_report(y_test_class, y_pred_class))\n",
    "\n",
    "    print(\"\\n--- Метки автокодировщика (аномалии) ---\")\n",
    "    print(confusion_matrix(y_test_class, y_pred_auto))\n",
    "    print(classification_report(y_test_class, y_pred_auto))\n",
    "\n",
    "    # Дополнительно: процент совпадений меток\n",
    "    agreement = np.mean(y_pred_class == y_pred_auto)\n",
    "    print(f\"\\nПроцент совпадений меток между классификатором и автокодировщиком: {agreement*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd54cccb-8cb8-4b47-8570-0bde255bb987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (классификатор): 0.9508\n",
      "Balanced Accuracy (автокодировщик): 0.6499\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "ROC-AUC (классификатор): 0.9972\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "ROC-AUC (автокодировщик, по ошибкам реконструкции): 0.9182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
    "bal_acc_clf = balanced_accuracy_score(y_test_class, y_pred_class)\n",
    "print(f\"Balanced Accuracy (классификатор): {bal_acc_clf:.4f}\")\n",
    "# --- Дополнительные метрики для автокодировщика ---\n",
    "bal_acc_auto = balanced_accuracy_score(y_test_class, y_pred_auto)\n",
    "print(f\"Balanced Accuracy (автокодировщик): {bal_acc_auto:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test_class)[:, 1]\n",
    "\n",
    "roc_auc_clf = roc_auc_score(y_test_class, y_pred_proba)\n",
    "print(f\"ROC-AUC (классификатор): {roc_auc_clf:.4f}\")\n",
    "\n",
    "anomaly_scores_test=get_anomaly_scores(final_autoencoder, X_test_class)\n",
    "\n",
    "roc_auc_auto = roc_auc_score(y_test_class, anomaly_scores_test)\n",
    "print(f\"ROC-AUC (автокодировщик, по ошибкам реконструкции): {roc_auc_auto:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6624b-67d6-453b-80fd-f003879cf6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_scores2(autoencoder, X, threshold_percentile=95):\n",
    "    \"\"\"Вычисляет ошибку реконструкции MSE для каждого окна.\"\"\"\n",
    "    reconstructions = autoencoder.predict(X)\n",
    "    mse = np.mean(np.square(X - reconstructions), axis=(1,2))\n",
    "    return reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7d279-7062-487e-8806-717d383f1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_anomaly_scores2(final_autoencoder, X_train_class).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443eb351-1e58-4eae-8454-16fc095c4936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
