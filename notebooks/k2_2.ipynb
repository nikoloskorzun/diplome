{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adcbf121-b7a5-4948-a35d-3e48f48e2f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "libs_path = os.path.abspath(os.path.join(current_dir, \"..\", \"libs\"))\n",
    "if libs_path not in sys.path:\n",
    "    sys.path.append(libs_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 3)\n",
    "import data_processing as dp\n",
    "import models as myML\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "026e2b04-a1c6-493a-84c4-c7b61d48765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Flatten, Dropout, RepeatVector, TimeDistributed, UpSampling1D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Установка seed для воспроизводимости\n",
    "np.random.seed(1337)\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = pd.read_excel(\"../datasets_ref/source.xls\")\n",
    "\n",
    "dataset[\"temp_Class\"] = np.uint(np.bool_(dataset[\"Class\"]))\n",
    "dataset[\"Class\"] = dataset[\"temp_Class\"]\n",
    "dataset = dataset.drop(columns=[\"temp_Class\"])\n",
    "\n",
    "dataset[\"power, W\"] = dataset[\"Ubs,V\"] * dataset[\"Ibs,A\"]\n",
    "dataset[\"D+\"] = np.sqrt(np.power(dataset[\"TR11,C\"],2) + np.power(dataset[\"TR13,C\"], 2) + np.power(dataset[\"TR15,C\"], 2))\n",
    "dataset[\"D-\"] = np.sqrt(np.power(dataset[\"TR12,C\"],2) + np.power(dataset[\"TR14,C\"], 2) + np.power(dataset[\"TR16,C\"], 2))\n",
    "dataset[\"D\"] = np.sqrt(np.power(dataset[\"D+\"],2) + np.power(dataset[\"D-\"], 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = dataset.drop(columns=['Class']).values\n",
    "y = dataset['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91ecf1-38af-4368-866b-dc38b0769a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import random\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# DEAP imports\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# ============================================================================\n",
    "# 1. FIXED ARCHITECTURE HYPERPARAMETERS (empirically set)\n",
    "# ============================================================================\n",
    "N_CONV_LAYERS = 2          # Fixed number of Conv1D layers\n",
    "N_RECURRENT_LAYERS = 2     # Fixed number of Bidirectional LSTM layers\n",
    "N_DENSE_LAYERS = 1         # Fixed number of dense layers in bottleneck/decoder\n",
    "WINDOW_SIZE = 50           # Fixed time series window size\n",
    "N_FEATURES = None          # Will be set after data loading\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "def create_windows(data, window_size=WINDOW_SIZE):\n",
    "    \"\"\"Create sliding windows from time series data.\"\"\"\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "    return np.array(X)\n",
    "\n",
    "def preprocess_data(df, target_col='class', test_ratio=0.2, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Preprocess dataset:\n",
    "    - Splits into train (unlabeled), validation (unlabeled), test (labeled)\n",
    "    - Normalizes using training statistics ONLY\n",
    "    - Ignores 'class' column during autoencoder training\n",
    "    \"\"\"\n",
    "    global N_FEATURES\n",
    "    \n",
    "    # Separate features (exclude target if exists)\n",
    "    if target_col in df.columns:\n",
    "        features = df.drop(columns=[target_col]).values\n",
    "        labels = df[target_col].values if target_col in df.columns else None\n",
    "    else:\n",
    "        features = df.values\n",
    "        labels = None\n",
    "    \n",
    "    N_FEATURES = features.shape[1]\n",
    "    \n",
    "    # Create windows\n",
    "    X_windows = create_windows(features)\n",
    "    \n",
    "    # Split indices (preserve temporal order)\n",
    "    n_total = len(X_windows)\n",
    "    n_test = int(n_total * test_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_train = n_total - n_test - n_val\n",
    "    \n",
    "    X_train = X_windows[:n_train]\n",
    "    X_val = X_windows[n_train:n_train + n_val]\n",
    "    X_test = X_windows[n_train + n_val:]\n",
    "    \n",
    "    # Extract labels for test set windows (use last timestep label)\n",
    "    y_test = None\n",
    "    if labels is not None:\n",
    "        y_test_windows = create_windows(labels.reshape(-1, 1))\n",
    "        y_test = y_test_windows[n_train + n_val:, -1, 0].astype(int)\n",
    "    \n",
    "    # Normalize using training statistics only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_flat = X_train.reshape(-1, N_FEATURES)\n",
    "    scaler.fit(X_train_flat)\n",
    "    \n",
    "    X_train_norm = scaler.transform(X_train_flat).reshape(X_train.shape)\n",
    "    X_val_norm = scaler.transform(X_val.reshape(-1, N_FEATURES)).reshape(X_val.shape)\n",
    "    X_test_norm = scaler.transform(X_test.reshape(-1, N_FEATURES)).reshape(X_test.shape)\n",
    "    \n",
    "    return (X_train_norm, X_val_norm, X_test_norm, y_test, scaler)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. AUTOENCODER MODEL BUILDER (parameterized)\n",
    "# ============================================================================\n",
    "def build_autoencoder(hparams):\n",
    "    \"\"\"\n",
    "    Build convolutional-Bidirectional LSTM autoencoder.\n",
    "    \n",
    "    hparams dict keys:\n",
    "        - conv_filters: list of ints (length = N_CONV_LAYERS)\n",
    "        - kernel_sizes: list of ints (length = N_CONV_LAYERS)\n",
    "        - lstm_units: list of ints (length = N_RECURRENT_LAYERS)\n",
    "        - dense_units: int (bottleneck size)\n",
    "        - optimizer: str ('adam', 'rmsprop', 'sgd')\n",
    "    \"\"\"\n",
    "    # ===== ENCODER =====\n",
    "    inputs = layers.Input(shape=(WINDOW_SIZE, N_FEATURES))\n",
    "    x = inputs\n",
    "    \n",
    "    # Convolutional layers\n",
    "    for i in range(N_CONV_LAYERS):\n",
    "        x = layers.Conv1D(\n",
    "            filters=hparams['conv_filters'][i],\n",
    "            kernel_size=hparams['kernel_sizes'][i],\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Recurrent layers\n",
    "    for i in range(N_RECURRENT_LAYERS):\n",
    "        return_sequences = (i < N_RECURRENT_LAYERS - 1)  # Only last layer returns single vector\n",
    "        x = layers.Bidirectional(\n",
    "            layers.LSTM(hparams['lstm_units'][i], return_sequences=return_sequences)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    encoded = layers.Dense(hparams['dense_units'], activation='relu', name='bottleneck')(x)\n",
    "    \n",
    "    # ===== DECODER =====\n",
    "    x = layers.RepeatVector(WINDOW_SIZE)(encoded)\n",
    "    \n",
    "    # Recurrent layers (mirror encoder)\n",
    "    for i in range(N_RECURRENT_LAYERS):\n",
    "        return_sequences = True\n",
    "        x = layers.Bidirectional(\n",
    "            layers.LSTM(hparams['lstm_units'][i], return_sequences=return_sequences)\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Convolutional transpose layers (approximate inverse)\n",
    "    for i in reversed(range(N_CONV_LAYERS)):\n",
    "        x = layers.Conv1D(\n",
    "            filters=hparams['conv_filters'][i],\n",
    "            kernel_size=hparams['kernel_sizes'][i],\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer (reconstruct original features)\n",
    "    outputs = layers.Conv1D(\n",
    "        filters=N_FEATURES,\n",
    "        kernel_size=3,\n",
    "        activation='linear',\n",
    "        padding='same'\n",
    "    )(x)\n",
    "    \n",
    "    # Build model\n",
    "    autoencoder = Model(inputs, outputs, name='ConvBiLSTM_Autoencoder')\n",
    "    \n",
    "    # Compile\n",
    "    opt = keras.optimizers.get(hparams['optimizer'])\n",
    "    autoencoder.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# ============================================================================\n",
    "# 4. GENETIC ALGORITHM WITH DEAP\n",
    "# ============================================================================\n",
    "def create_individual():\n",
    "    \"\"\"Create random individual representing hyperparameter set.\"\"\"\n",
    "    return [\n",
    "        random.choice([16, 32, 64]),           # conv_filters layer 1\n",
    "        random.choice([16, 32, 64]),           # conv_filters layer 2\n",
    "        random.choice([3, 5, 7]),              # kernel_size layer 1\n",
    "        random.choice([3, 5, 7]),              # kernel_size layer 2\n",
    "        random.choice([32, 64, 128]),          # lstm_units layer 1\n",
    "        random.choice([32, 64, 128]),          # lstm_units layer 2\n",
    "        random.choice([16, 32, 64]),           # dense_units bottleneck\n",
    "        random.choice(['adam', 'rmsprop'])     # optimizer\n",
    "    ]\n",
    "\n",
    "def individual_to_hparams(ind):\n",
    "    \"\"\"Convert DEAP individual to hyperparameter dictionary.\"\"\"\n",
    "    return {\n",
    "        'conv_filters': [ind[0], ind[1]],\n",
    "        'kernel_sizes': [ind[2], ind[3]],\n",
    "        'lstm_units': [ind[4], ind[5]],\n",
    "        'dense_units': ind[6],\n",
    "        'optimizer': ind[7]\n",
    "    }\n",
    "\n",
    "def evaluate_individual(ind, X_train, X_val, epochs=15, batch_size=64):\n",
    "    \"\"\"\n",
    "    Fitness function: negative validation MSE (higher fitness = better)\n",
    "    Trains autoencoder for limited epochs to evaluate hyperparameters.\n",
    "    \"\"\"\n",
    "    hparams = individual_to_hparams(ind)\n",
    "    \n",
    "    try:\n",
    "        # Build and train model\n",
    "        model = build_autoencoder(hparams)\n",
    "        \n",
    "        # Early stopping to speed up evaluation\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)\n",
    "        ]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, X_train,\n",
    "            validation_data=(X_val, X_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fitness = negative validation loss (maximization problem)\n",
    "        fitness = -history.history['val_loss'][-1]\n",
    "        \n",
    "        # Clean up to avoid memory leaks\n",
    "        tf.keras.backend.clear_session()\n",
    "        del model\n",
    "        \n",
    "        return (fitness,)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Training failed for individual {ind}: {str(e)}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        return (-1e6,)  # Very bad fitness\n",
    "\n",
    "def setup_deap_toolbox(X_train, X_val):\n",
    "    \"\"\"Configure DEAP toolbox for hyperparameter optimization.\"\"\"\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"evaluate\", evaluate_individual, X_train=X_train, X_val=X_val)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutUniformInt, \n",
    "                     low=[16,16,3,3,32,32,16,0], \n",
    "                     up=[64,64,7,7,128,128,64,1], \n",
    "                     indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    \n",
    "    return toolbox\n",
    "\n",
    "def run_genetic_optimization(X_train, X_val, pop_size=30, n_gen=20):\n",
    "    \"\"\"Execute GA to find optimal hyperparameters.\"\"\"\n",
    "    print(f\"\\n[GA] Starting hyperparameter optimization ({pop_size} individuals, {n_gen} generations)...\")\n",
    "    \n",
    "    toolbox = setup_deap_toolbox(X_train, X_val)\n",
    "    population = toolbox.population(n=pop_size)\n",
    "    \n",
    "    # Evaluate initial population\n",
    "    fitnesses = map(toolbox.evaluate, population)\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # Evolution loop\n",
    "    for gen in range(n_gen):\n",
    "        print(f\"  Generation {gen+1}/{n_gen} | Best fitness: {max(ind.fitness.values[0] for ind in population):.6f}\")\n",
    "        \n",
    "        # Select parents\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # Crossover\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < 0.7:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        \n",
    "        # Mutation\n",
    "        for mutant in offspring:\n",
    "            if random.random() < 0.2:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # Evaluate invalid individuals\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        # Replace population\n",
    "        population[:] = offspring\n",
    "    \n",
    "    # Return best individual\n",
    "    best_ind = tools.selBest(population, 1)[0]\n",
    "    print(f\"\\n[GA] Optimization complete. Best fitness: {best_ind.fitness.values[0]:.6f}\")\n",
    "    print(f\"Best hyperparameters: {best_ind}\")\n",
    "    \n",
    "    return individual_to_hparams(best_ind)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ANOMALY DETECTION PIPELINE\n",
    "# ============================================================================\n",
    "def train_final_autoencoder(hparams, X_train, X_val, epochs=100, batch_size=64):\n",
    "    \"\"\"Train final autoencoder with best hyperparameters.\"\"\"\n",
    "    print(\"\\n[Autoencoder] Training final model with optimized hyperparameters...\")\n",
    "    model = build_autoencoder(hparams)\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "        keras.callbacks.ModelCheckpoint('best_autoencoder.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, X_train,\n",
    "        validation_data=(X_val, X_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Load best weights\n",
    "    model.load_weights('best_autoencoder.h5')\n",
    "    return model, history\n",
    "\n",
    "def compute_reconstruction_errors(model, X):\n",
    "    \"\"\"Compute MSE reconstruction error for each window.\"\"\"\n",
    "    reconstructions = model.predict(X, verbose=0)\n",
    "    errors = np.mean(np.square(X - reconstructions), axis=(1, 2))  # Mean over timesteps and features\n",
    "    return errors\n",
    "\n",
    "def determine_anomaly_threshold(errors, percentile=95):\n",
    "    \"\"\"Determine threshold using percentile of reconstruction errors.\"\"\"\n",
    "    threshold = np.percentile(errors, percentile)\n",
    "    print(f\"[Threshold] Using {percentile}th percentile: {threshold:.6f}\")\n",
    "    return threshold\n",
    "\n",
    "def detect_anomalies(model, X, threshold):\n",
    "    \"\"\"Detect anomalies based on reconstruction error threshold.\"\"\"\n",
    "    errors = compute_reconstruction_errors(model, X)\n",
    "    anomalies = (errors > threshold).astype(int)\n",
    "    return anomalies, errors\n",
    "\n",
    "# ============================================================================\n",
    "# 6. CLASSIFIER (Bidirectional LSTM Bagging Ensemble)\n",
    "# ============================================================================\n",
    "def build_classifier(input_shape, lstm_units=64, dense_units=32):\n",
    "    \"\"\"Build single Bidirectional LSTM classifier.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(inputs)\n",
    "    x = layers.Bidirectional(layers.LSTM(lstm_units))(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_bagging_ensemble(X_train, y_train, n_estimators=5, epochs=50):\n",
    "    \"\"\"Train bagging ensemble of classifiers on bootstrap samples.\"\"\"\n",
    "    print(f\"\\n[Classifier] Training bagging ensemble ({n_estimators} models)...\")\n",
    "    models = []\n",
    "    n_samples = len(X_train)\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        # Bootstrap sampling\n",
    "        idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_boot, y_boot = X_train[idx], y_train[idx]\n",
    "        \n",
    "        model = build_classifier((WINDOW_SIZE, N_FEATURES))\n",
    "        model.fit(\n",
    "            X_boot, y_boot,\n",
    "            epochs=epochs,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "            callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "        )\n",
    "        models.append(model)\n",
    "        print(f\"  Trained model {i+1}/{n_estimators}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def ensemble_predict(models, X):\n",
    "    \"\"\"Aggregate predictions from ensemble members.\"\"\"\n",
    "    preds = np.zeros((len(X),))\n",
    "    for model in models:\n",
    "        preds += model.predict(X, verbose=0).flatten()\n",
    "    return (preds / len(models) > 0.5).astype(int)\n",
    "\n",
    "# ============================================================================\n",
    "# 7. EVALUATION & COMPARISON\n",
    "# ============================================================================\n",
    "def evaluate_anomaly_detection(y_true, y_pred_autoencoder, y_pred_classifier):\n",
    "    \"\"\"Compare autoencoder vs classifier performance.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANOMALY DETECTION EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Autoencoder metrics\n",
    "    p_ae, r_ae, f1_ae, _ = precision_recall_fscore_support(y_true, y_pred_autoencoder, average='binary')\n",
    "    auc_ae = roc_auc_score(y_true, y_pred_autoencoder)\n",
    "    cm_ae = confusion_matrix(y_true, y_pred_autoencoder)\n",
    "    \n",
    "    # Classifier metrics\n",
    "    p_clf, r_clf, f1_clf, _ = precision_recall_fscore_support(y_true, y_pred_classifier, average='binary')\n",
    "    auc_clf = roc_auc_score(y_true, y_pred_classifier)\n",
    "    cm_clf = confusion_matrix(y_true, y_pred_classifier)\n",
    "    \n",
    "    print(\"\\nAutoencoder (Unsupervised Anomaly Detection):\")\n",
    "    print(f\"  Precision: {p_ae:.4f} | Recall: {r_ae:.4f} | F1: {f1_ae:.4f} | AUC: {auc_ae:.4f}\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(cm_ae)\n",
    "    \n",
    "    print(\"\\nBidirectional LSTM Bagging Ensemble (Supervised Classifier):\")\n",
    "    print(f\"  Precision: {p_clf:.4f} | Recall: {r_clf:.4f} | F1: {f1_clf:.4f} | AUC: {auc_clf:.4f}\")\n",
    "    print(\"  Confusion Matrix:\")\n",
    "    print(cm_clf)\n",
    "    \n",
    "    print(\"\\nKey Insight:\")\n",
    "    print(\"  Autoencoder detects deviations from normal patterns without labels.\")\n",
    "    print(\"  Classifier leverages labeled anomalies but requires supervision.\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 8. MAIN PIPELINE\n",
    "# ============================================================================\n",
    "def main_pipeline(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline:\n",
    "    1. Preprocess data (ignores 'class' during autoencoder training)\n",
    "    2. Optimize hyperparameters via GA\n",
    "    3. Train final autoencoder on unlabeled data\n",
    "    4. Determine anomaly threshold\n",
    "    5. Train classifier ensemble on labeled portion (for comparison only)\n",
    "    6. Evaluate both approaches on test set\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"SPACECRAFT TELEMETRY ANOMALY DETECTION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Preprocess\n",
    "    X_train, X_val, X_test, y_test, scaler = preprocess_data(df, target_col=target_col)\n",
    "    print(f\"\\n[Data] Shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    if y_test is not None:\n",
    "        print(f\"[Data] Test set anomalies: {np.sum(y_test)} / {len(y_test)} ({np.mean(y_test)*100:.2f}%)\")\n",
    "    \n",
    "    # Step 2: Hyperparameter optimization (using only unlabeled data)\n",
    "    best_hparams = run_genetic_optimization(X_train, X_val)\n",
    "    \n",
    "    # Step 3: Train final autoencoder\n",
    "    autoencoder, history = train_final_autoencoder(best_hparams, X_train, X_val)\n",
    "    \n",
    "    # Step 4: Determine anomaly threshold on validation set\n",
    "    val_errors = compute_reconstruction_errors(autoencoder, X_val)\n",
    "    threshold = determine_anomaly_threshold(val_errors, percentile=95)\n",
    "    \n",
    "    # Step 5: Detect anomalies on test set\n",
    "    y_pred_ae, test_errors = detect_anomalies(autoencoder, X_test, threshold)\n",
    "    \n",
    "    # Step 6: Train classifier ensemble (ONLY for comparison - uses labels)\n",
    "    if y_test is not None:\n",
    "        # Create training labels for classifier (using last timestep of each window)\n",
    "        y_train_windows = create_windows(df[target_col].values.reshape(-1, 1))\n",
    "        y_train = y_train_windows[:len(X_train), -1, 0].astype(int)\n",
    "        \n",
    "        # Train ensemble\n",
    "        classifier_ensemble = train_bagging_ensemble(X_train, y_train, n_estimators=5)\n",
    "        y_pred_clf = ensemble_predict(classifier_ensemble, X_test)\n",
    "        \n",
    "        # Step 7: Evaluate both approaches\n",
    "        evaluate_anomaly_detection(y_test, y_pred_ae, y_pred_clf)\n",
    "    else:\n",
    "        print(\"\\n⚠️ No labels available for test set - skipping classifier comparison.\")\n",
    "        print(f\"Detected {np.sum(y_pred_ae)} anomalies in test set using autoencoder.\")\n",
    "    \n",
    "    # Cleanup\n",
    "    tf.keras.backend.clear_session()\n",
    "    print(\"\\n[Pipeline] Completed successfully.\")\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE (user loads dataset themselves)\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # User should load their dataset here:\n",
    "    # import pandas as pd\n",
    "    # df = pd.read_csv('your_telemetry_dataset.csv')\n",
    "    # \n",
    "    # Then run:\n",
    "    main_pipeline(dataset, target_col='Сlass')\n",
    "    #\n",
    "    # Requirements:\n",
    "    #   pip install numpy tensorflow scikit-learn deap\n",
    "    \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
